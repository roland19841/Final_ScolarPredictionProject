# ğŸ“ School Success Prediction â€” Industrialized ML Project

## ğŸ“Œ Overview

This project delivers an **end-to-end industrialized Machine Learning application** for predicting student success.
It demonstrates the full lifecycle of a data product:

- Model training and evaluation
- Model versioning and tracking
- REST API for inference
- Web-based user interface
- Observability and monitoring
- Reproducible deployment with Docker

The project is aligned with **MLOps best practices** and suitable for an academic or professional evaluation.

---

## ğŸ§  Use Case

Predict whether a student is likely to **succeed or fail** based on socioâ€‘educational indicators  
(using **Scenario 3** from the *Student Performance* dataset).

- Target: binary success indicator
- Prediction returned with probability
- Scenario 3 excludes final grade (G3) from inputs

---

## ğŸ§© Technical Stack

| Layer | Technology |
|-----|-----------|
| API | FastAPI |
| IHM | Streamlit |
| ML | scikit-learn |
| Tracking | MLflow |
| Serialization | joblib |
| Containerization | Docker / Docker Compose |
| Language | Python 3.11 |

---

## ğŸ“‚ Project Structure

```
SCOLAR_PREDICTION_PROJECT/
â”œâ”€â”€ api_app/                # FastAPI application
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ihm_app/                # Streamlit interface
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ artifacts/              # ML artifacts
â”‚   â”œâ”€â”€ scenario3_features.json
â”‚   â””â”€â”€ models/
â”‚
â”œâ”€â”€ logs/                   # Inference logs (JSONL)
â”‚
â”œâ”€â”€ mlruns/                 # MLflow runs (file store)
â”‚
â”œâ”€â”€ data/                   # Datasets (CSV)
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Execution

### Option 1 â€” Run locally (without Docker)

#### 1. Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # Linux / Mac
.venv\Scripts\activate    # Windows
```

#### 2. Install dependencies
```bash
pip install -r api_app/requirements.txt
pip install -r ihm_app/requirements.txt
```

#### 3. Start the API
```bash
uvicorn api_app.main:app --reload --port 8000
```

#### 4. Start the IHM
```bash
streamlit run ihm_app/app.py
```

#### 5. (Optional) Start MLflow UI
```bash
mlflow ui --port 5000
```

Access:
- API docs: http://localhost:8000/docs
- IHM: http://localhost:8501
- MLflow UI: http://localhost:5000

---

### Option 2 â€” Run with Docker (recommended)

This is the **preferred method** for evaluation.

#### Prerequisites
- Docker
- Docker Compose

#### 1. Build & start all services
```bash
docker compose up --build
```

#### 2. Access services
- IHM (Streamlit): http://localhost:8501
- API (FastAPI Swagger): http://localhost:8000/docs
- MLflow UI: http://localhost:5000

Everything runs with **one command**, no Python installation required.

---

## ğŸ” Machine Learning Workflow

### Training (`POST /train`)
- Loads dataset from `data/`
- Performs train/test split + cross-validation
- Computes accuracy and F1-score
- Trains final model on full dataset
- Saves:
  - Versioned model
  - Training report (`train_report.json`)
- Logs run in MLflow (params, metrics, artifacts)

### Prediction (`POST /predict`)
- Validates input features
- Applies trained pipeline
- Returns:
  - Prediction (0 / 1)
  - Probability of success
- Logs inference in `logs/inference_log.jsonl`

---

## ğŸ“Š MLflow Usage

MLflow is used **only for training runs**, not for predictions.

Each training:
- Creates one MLflow run
- Logs parameters, metrics, artifacts
- Stores runs in `mlruns/` (file-based store)

The MLflow UI allows:
- Comparing experiments
- Inspecting metrics
- Downloading models and reports

---

## ğŸ§ª Monitoring & Observability

### `/health` endpoint
Provides:
- API status
- Model loaded or not
- Uptime
- Last training metrics
- Last inference event

### Inference logs
All predictions are logged in:
```
logs/inference_log.jsonl
```

Each line contains:
- Timestamp
- Endpoint
- User ID
- Input payload
- Output prediction

---

## ğŸ“„ Input Contract

The expected input features are defined in:
```
artifacts/scenario3_features.json
```

This file is shared by:
- API (validation)
- IHM (form generation)

It guarantees **APIâ€“UI consistency**.

---

## âœ… Key Deliverables

- âœ” Industrialized ML pipeline
- âœ” REST API with validation & monitoring
- âœ” Web interface
- âœ” MLflow experiment tracking
- âœ” Dockerized deployment
- âœ” Clean repository structure

---

## ğŸ‘¤ Author

Project developed as part of an **AI / IT Expert certification deliverable**.

---

## ğŸ“ Notes for Evaluation

- Use Docker for fastest evaluation
- Train the model via Swagger or IHM
- Inspect runs in MLflow UI
- Check logs and artifacts for traceability
